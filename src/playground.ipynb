{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sentence_similarity import SentenceSimilarity\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "load_dotenv()\n",
    "\n",
    "date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# with open(f'../data/newsdataio/sentiment/{date}.json', 'r') as f:\n",
    "with open(f'../data/newsdataio/filtered/{yesterday}.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do sentence similarity\n",
    "sentence_similarity = SentenceSimilarity(data)\n",
    "sentence_similarity.run()\n",
    "similar_articles = getattr(sentence_similarity, '_SentenceSimilarity__similar_articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort similar articles list in descending order\n",
    "similar_articles.sort(key=lambda x: len(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "analyzed_articles = deepcopy(similar_articles)\n",
    "len(analyzed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_articles = deepcopy(analyzed_articles)\n",
    "len(similar_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 0 and 1\n",
      "Combined 0 and 2\n",
      "Combined 0 and 3\n",
      "Combined 0 and 5\n",
      "Combined 0 and 12\n",
      "Number of categories: 87\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(similar_articles)):\n",
    "    for j in range(i+1, len(similar_articles)):\n",
    "        i_articles = set(similar_articles[i])\n",
    "        j_articles = set(similar_articles[j])\n",
    "\n",
    "        if not j_articles:\n",
    "            continue\n",
    "\n",
    "        if i_articles.intersection(j_articles):\n",
    "            similar_articles[i] += similar_articles[j]\n",
    "            similar_articles[j] = []\n",
    "            print(\"Combined {} and {}\".format(i, j))\n",
    "\n",
    "new_art = [x for x in similar_articles if x]\n",
    "print(\"Number of categories: {}\".format(len(new_art)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 21, 30, 31, 43, 46, 50, 60, 62, 112, 116, 124, 133, 140, 149, 150, 162, 165, 169, 179, 181, 231, 235, 243, 252, 253, 265, 268, 272, 282, 284, 0, 30, 43, 60, 62, 103, 109, 112, 149, 162, 179, 181, 217, 222, 228, 231, 252, 265, 282, 284, 98, 21, 46, 60, 112, 116, 119, 123, 140, 165, 179, 231, 235, 236, 238, 242, 243, 268, 282, 117, 43, 46, 50, 60, 158, 162, 165, 169, 179, 261, 265, 268, 272, 282, 39, 9, 31, 133, 146, 150, 249, 253, 27, 119, 207, 238, 88], [6, 25, 33, 77, 125, 130, 144, 152, 196, 247, 255, 1], [28, 65, 147, 184, 250, 287, 16], [34, 142, 153, 245, 256, 23], [42, 157, 161, 260, 264, 38], [59, 159, 178, 262, 281, 40], [64, 180, 183, 283, 286, 61], [102, 141, 221, 244, 22], [121, 210, 240, 91], [106, 218, 225, 99], [143, 246, 24], [145, 248, 26], [148, 251, 29], [151, 254, 32], [154, 257, 35], [155, 258, 36], [156, 259, 37], [160, 263, 41], [163, 266, 44], [164, 267, 45], [166, 269, 47], [167, 270, 48], [168, 271, 49], [170, 273, 51], [171, 274, 52], [172, 275, 53], [173, 276, 54], [174, 277, 55], [175, 278, 56], [176, 279, 57], [177, 280, 58], [182, 285, 63], [126, 2], [127, 3], [128, 4], [129, 5], [131, 7], [132, 8], [134, 10], [135, 11], [136, 12], [137, 13], [138, 14], [139, 15], [185, 66], [186, 67], [187, 68], [188, 69], [189, 70], [190, 71], [191, 72], [192, 73], [193, 74], [194, 75], [195, 76], [197, 78], [198, 79], [199, 80], [200, 81], [201, 82], [202, 83], [203, 84], [204, 85], [205, 86], [206, 87], [208, 89], [209, 90], [211, 92], [212, 93], [213, 94], [214, 95], [215, 96], [216, 97], [219, 100], [220, 101], [223, 104], [224, 105], [226, 107], [227, 108], [229, 110], [230, 111], [232, 113], [233, 114], [234, 115], [237, 118], [239, 120], [241, 122]]\n"
     ]
    }
   ],
   "source": [
    "print(new_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143    GST 2.0 Will Further Ease Tax Compliances And ...\n",
       "246    GST 2.0 Will Further Ease Tax Compliances And ...\n",
       "24     GST 2.0 Will Further Ease Tax Compliances And ...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_to_check = new_art[10]\n",
    "# for all elements in articles_to_check print the title from df as row items\n",
    "pd.DataFrame([df.iloc[i] for i in articles_to_check]).title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vrushank/newsAggregator/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "DEFAULT_MODEL = \"google/gemma-2-2b-it\"\n",
    "MODEL_TEMPERATURE = 0.3\n",
    "MAX_TOKENS = 4096\n",
    "TOP_P = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(api_key=os.getenv('HUGGINGFACE_READ_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Rise of First Indian Defence Dragon: Big Bang Boom Solutions Raises Rs 250 Crore with a 15x Return to Early Investors\",\n",
    "    \"Sensex, Nifty touch all-time closing high levels amid strong foreign fund inflows\",\n",
    "    \"Yashasvi Jaiswal Eyes Record Breaking Feat As India Gears Up For 2nd Test Against Bangladesh\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"\"\"\n",
    "Using the rule of three, convert these three headline into catchy three word titles.\n",
    "Do not use the same word in any of the titles.\n",
    "\n",
    "1. {titles[0]}\n",
    "2. {titles[1]}\n",
    "3. {titles[2]}\n",
    "\n",
    "Return the response as a JSON dictionary.\n",
    "The JSON dictionary should have only one output entry.\n",
    "The dictionary output should only have three keys as 'one', 'two' and 'three'.\n",
    "Each key should have the summary of the corresponding headline.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role': 'user', 'content': input}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/google/gemma-2-2b-it/v1/chat/completions (Request ID: J93xa69CDrG9EdO_aovh6)\n\nModel too busy, unable to get response in less than 60 second(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/newsAggregator/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/newsAggregator/.venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/google/gemma-2-2b-it/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEFAULT_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_TEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_P\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newsAggregator/.venv/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:842\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    821\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    822\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    823\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    839\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    840\u001b[0m )\n\u001b[1;32m    841\u001b[0m payload \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 842\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/newsAggregator/.venv/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:305\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/newsAggregator/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/google/gemma-2-2b-it/v1/chat/completions (Request ID: J93xa69CDrG9EdO_aovh6)\n\nModel too busy, unable to get response in less than 60 second(s)"
     ]
    }
   ],
   "source": [
    "output = client.chat.completions.create(\n",
    "    model = DEFAULT_MODEL,\n",
    "    messages = messages,\n",
    "    temperature=MODEL_TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='```json\\n{\\n  \"one\": \"Indian Defence Boom\",\\n  \"two\": \"Market Hits Record\",\\n  \"three\": \"Jaiswal Sets Record\"\\n}\\n``` \\n', tool_calls=None), logprobs=None)], created=1727205452, id='', model='google/gemma-2-2b-it', system_fingerprint='2.2.1-dev0-sha-e415b69', usage=ChatCompletionOutputUsage(completion_tokens=43, prompt_tokens=165, total_tokens=208))\n",
      "<class 'huggingface_hub.inference._generated.types.chat_completion.ChatCompletionOutput'>\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"one\": \"Indian Defence Boom\",\n",
      "  \"two\": \"Market Hits Record\",\n",
      "  \"three\": \"Jaiswal Sets Record\"\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the content of the assistant's response\n",
    "json_output = output['choices'][0]['message']['content']\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"one\": \"Indian Defence Boom\",\n",
      "  \"two\": \"Market Hits Record\",\n",
      "  \"three\": \"Jaiswal Sets Record\"\n",
      "}\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert this json output into list of summaries\n",
    "json_output = json_output.replace('`', '')\n",
    "json_output = json_output.replace('json', '')\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indian Defence Boom', 'Market Hits Record', 'Jaiswal Sets Record']\n"
     ]
    }
   ],
   "source": [
    "json_output = json.loads(json_output)\n",
    "summaries = [json_output[key] for key in json_output]\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian Defence Boom, Market Hits Record, and Jaiswal Sets Record\n"
     ]
    }
   ],
   "source": [
    "# concatenate the summary as a single string as {one}, {two} and {three}\n",
    "summary = summaries[0] + ', ' + summaries[1] + ', and ' + summaries[2]\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
